healthCheckTimeout: 500

logLevel: info

startPort: 10001

sendLoadingState: true

includeAliasesInList: false

models:
  qwen3_coder_reap_25b_A3B_IQ4_XS:
    cmd: /app/llama-server --port ${PORT} -m /models/Qwen3-Coder-REAP-25B-A3B.IQ4_XS.gguf -ngl 99 --ctx-size 200000 --n-gpu-layers 49 -cmoe
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health
    name: "Qwen3 Coder Reap 25B IQ4_XS"
    description: "A large language model for coding tasks"

  qwen3_vl_30b_instruct:
    cmd: /app/llama-server --port ${PORT} -m /models/Qwen3-VL-30B-A3B-Instruct-UD-Q5_K_XL.gguf -ngl 99 --mmproj /models/mmproj-BF16.gguf --ctx-size 64000 --n-gpu-layers 33
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health
    name: "Qwen3 VL 30B Instruct"
    description: "A vision-language model for multimodal tasks"

#  --cache-type-k q8_0 --cache-type-v q8_
# 0

  qwen3_vl_30b_thinking:
    cmd: /app/llama-server --port ${PORT} -m /models/Qwen3-VL-30B-A3B-Thinking-UD-Q4_K_XL.gguf --mmproj /models/mmproj-thinking-BF16.gguf -ngl 99 --ctx-size 32000 --n-gpu-layers 33
    proxy: http://127.0.0.1:${PORT}
    checkEndpoint: /health
    name: "Qwen3 VL 30B Instruct"
    description: "A vision-language model for multimodal tasks"

  vibeThinker:
    cmd: /app/llama-server --port ${PORT} -m /models/VibeThinker-1.5B.Q8_0.gguf?download=true  -ngl 99 --ctx-size 128000 --n-gpu-layers 33

  Qwen3-4B-Instruct-2507-vllm:
    cmd: /usr/local/bin/vllm serve /models/Qwen3-4B-Instruct-2507 --max-model-len 8192 --port ${PORT} --served-model-name Qwen3-4B-Instruct-2507-vllm

# ADD THIS SECTION:
groups:
  main_group:
    swap: true           # Enable swapping within this group
    exclusive: false     # If true, stops ALL other groups when this runs
    persistent: false    # If true, never auto-unload this group
    members:
      - qwen3_coder_reap_25b_A3B_IQ4_XS
      - qwen3_vl_30b_instruct
      - vibeThinker
      - qwen3_vl_30b_thinking
      - Qwen3-4B-Instruct-2507-vllm